[
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "chromadb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chromadb",
        "description": "chromadb",
        "detail": "chromadb",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "embed_text",
        "kind": 2,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "def embed_text(text):\n    emb = client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=text\n    )\n    return emb.data[0].embedding\n# ====== SAVE MEMORY ======\ndef save_memory(user_message, agent_reply):\n    combined = f\"User: {user_message}\\nAgent: {agent_reply}\"\n    embedding = embed_text(combined)",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "save_memory",
        "kind": 2,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "def save_memory(user_message, agent_reply):\n    combined = f\"User: {user_message}\\nAgent: {agent_reply}\"\n    embedding = embed_text(combined)\n    collection.add(\n        ids=[str(np.random.randint(1_000_000_000))],\n        embeddings=[embedding],\n        documents=[combined]\n    )\n    print(\"ðŸ’¾ Memory saved.\")\n# ====== RETRIEVE MEMORY ======",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "retrieve_memory",
        "kind": 2,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "def retrieve_memory(query, top_k=3):\n    query_embedding = embed_text(query)\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=top_k\n    )\n    docs = results.get(\"documents\", [])\n    # Flatten and filter out empty\n    flat_docs = [doc for sublist in docs for doc in sublist if doc]\n    return flat_docs",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "chat_with_memory",
        "kind": 2,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "def chat_with_memory(user_input):\n    relevant_memories = retrieve_memory(user_input)\n    context = \"\\n\".join(relevant_memories) if relevant_memories else \"No past memories.\"\n    prompt = f\"Context from memory:\\n{context}\\n\\nUser: {user_input}\\nAgent:\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    agent_reply = response.choices[0].message.content\n    save_memory(user_input, agent_reply)",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "CHROMA_API_KEY",
        "kind": 5,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "CHROMA_API_KEY = os.getenv(\"CHROMA_API_KEY\")\nCHROMA_TENANT = os.getenv(\"CHROMA_TENANT\")\nCHROMA_DATABASE = os.getenv(\"CHROMA_DATABASE\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n# ====== INIT ======\nclient = OpenAI(api_key=OPENAI_API_KEY)\nchroma_client = chromadb.CloudClient(\n    api_key=CHROMA_API_KEY,\n    tenant=CHROMA_TENANT,\n    database=CHROMA_DB",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "CHROMA_TENANT",
        "kind": 5,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "CHROMA_TENANT = os.getenv(\"CHROMA_TENANT\")\nCHROMA_DATABASE = os.getenv(\"CHROMA_DATABASE\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n# ====== INIT ======\nclient = OpenAI(api_key=OPENAI_API_KEY)\nchroma_client = chromadb.CloudClient(\n    api_key=CHROMA_API_KEY,\n    tenant=CHROMA_TENANT,\n    database=CHROMA_DB\n)",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "CHROMA_DATABASE",
        "kind": 5,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "CHROMA_DATABASE = os.getenv(\"CHROMA_DATABASE\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n# ====== INIT ======\nclient = OpenAI(api_key=OPENAI_API_KEY)\nchroma_client = chromadb.CloudClient(\n    api_key=CHROMA_API_KEY,\n    tenant=CHROMA_TENANT,\n    database=CHROMA_DB\n)\ncollection = chroma_client.get_or_create_collection(\"long_term_memory\")",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "OPENAI_API_KEY",
        "kind": 5,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n# ====== INIT ======\nclient = OpenAI(api_key=OPENAI_API_KEY)\nchroma_client = chromadb.CloudClient(\n    api_key=CHROMA_API_KEY,\n    tenant=CHROMA_TENANT,\n    database=CHROMA_DB\n)\ncollection = chroma_client.get_or_create_collection(\"long_term_memory\")\n# ====== EMBEDDING ======",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "client = OpenAI(api_key=OPENAI_API_KEY)\nchroma_client = chromadb.CloudClient(\n    api_key=CHROMA_API_KEY,\n    tenant=CHROMA_TENANT,\n    database=CHROMA_DB\n)\ncollection = chroma_client.get_or_create_collection(\"long_term_memory\")\n# ====== EMBEDDING ======\ndef embed_text(text):\n    emb = client.embeddings.create(",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "chroma_client",
        "kind": 5,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "chroma_client = chromadb.CloudClient(\n    api_key=CHROMA_API_KEY,\n    tenant=CHROMA_TENANT,\n    database=CHROMA_DB\n)\ncollection = chroma_client.get_or_create_collection(\"long_term_memory\")\n# ====== EMBEDDING ======\ndef embed_text(text):\n    emb = client.embeddings.create(\n        model=\"text-embedding-3-small\",",
        "detail": "long_term_memory",
        "documentation": {}
    },
    {
        "label": "collection",
        "kind": 5,
        "importPath": "long_term_memory",
        "description": "long_term_memory",
        "peekOfCode": "collection = chroma_client.get_or_create_collection(\"long_term_memory\")\n# ====== EMBEDDING ======\ndef embed_text(text):\n    emb = client.embeddings.create(\n        model=\"text-embedding-3-small\",\n        input=text\n    )\n    return emb.data[0].embedding\n# ====== SAVE MEMORY ======\ndef save_memory(user_message, agent_reply):",
        "detail": "long_term_memory",
        "documentation": {}
    }
]